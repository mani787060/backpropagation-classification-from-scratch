# Backpropagation Classification From Scratch

This project demonstrates the implementation of the **backpropagation algorithm for a classification problem from scratch**, without using any deep learning frameworks.

The notebook focuses on understanding how neural networks learn through gradient-based optimization.

---

## Topics Covered
- Artificial Neural Networks (ANN)
- Forward Propagation
- Backpropagation Algorithm
- Gradient Descent
- Binary Classification
- Loss Function Computation

---

## Learning Objective
- Understand how errors are propagated backward in neural networks
- Learn manual gradient computation and weight updates
- Build strong conceptual foundations before using DL frameworks

---

## Tech Stack
- Python
- NumPy

---

## Project Purpose
This notebook is part of my **Deep Learning learning journey**, aimed at building models from first principles to strengthen core understanding of neural networks.
